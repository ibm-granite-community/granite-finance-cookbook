{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Entity Extraction from IBM's Quaterly Earning Transcript call using Granite-8B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook works with two approaches to extract the entities from the transcript. The first approach is defining the entities in the prompt directly along with its description. In the second approach, we are defining the entities in a class and then converting it into pydantic function. This is then passed along with the prompt to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model used in this notebook is IBM's Granite-8b-preview-4k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the langchain libraries for prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this model is currently only avaiable on BAM, we have to do BAM API call.  \n",
    "All imports requried for doing BAM API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai import Client, Credentials\n",
    "from genai.schema import TextGenerationParameters\n",
    "from genai.extensions.langchain import LangChainInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching the BAM credentials from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_url = os.environ['BAM_URL']\n",
    "bam_api = os.environ['BAM_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the parameters for the model to make it give accurate output. Model used here is IBM's granite-8b-preview-4k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMP**: You need to have access to the granite-8b-preview-4k model on BAM to be able to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = TextGenerationParameters(\n",
    "    decoding_method=\"sampling\",\n",
    "    max_new_tokens=100,\n",
    "    min_new_tokens=50,\n",
    "    reptition_penalty = 1,\n",
    "    temperature=0,\n",
    "    top_k= 1\n",
    "    # random_seed=999\n",
    "    )\n",
    "creds = Credentials(api_endpoint=bam_url, api_key=bam_api)\n",
    "client = Client(credentials=creds)\n",
    "model_id = 'ibm/granite-8b-instruct-preview-4k'\n",
    "\n",
    "llm = LangChainInterface(model_id=model_id, client=client, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching and displaying the transcript from a pre-loaded .txt file present in the same parent folder as this jupyter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ibm_transcript.txt', 'r') as file:\n",
    "    page_content = file.read()\n",
    "print(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - All the entities defined in the prompt only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the entities that needs to be fetched is defined in the prompt itself along with the entitie's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_prompt_only():\n",
    "    template = \"\"\"\n",
    "    <|start_of_role|>user<|end_of_role|>\n",
    "    -You are AI Entity Extractor. You help extracting entities from the given transcript: {page_content}\n",
    "    -Analyze this transcript and extract the following entities:\n",
    "    1) `company_name` : This is the name of the company for which the transcript is given.\n",
    "    2) `ceo_name`: This is the name of the CEO of IBM.\n",
    "    3) `pre_tax_profit_percentage`: This is the operating pre-tax profit in percentage.\n",
    "    4) `pre_tax_profit_number`: This is the operating pre-tax profit in numbers.\n",
    "    5) `total_revenue_transaction_processing`: This is the total revenue growth for Transaction Processing sector in percentage.\n",
    "    6) `total_revenue_data_ai`: This is the total revenue growth for Data and AI sector in percentage.\n",
    "    7) `total_revenue_security`: This is the total revenue growth/decline for security sector in percentage.\n",
    "    8) `total_revenue_automation`: This is total revenue growth/decline for automation sector in percentage.\n",
    "    9) `names_of_people`: All the names of the people that were mentioned in the transcript. Do not assume the last name. Only output the names which you find in the transcript.\n",
    "    \n",
    "    -Your output should strictly be in a json format.\n",
    "    -If any entity is not found, your output shound be `data not available`. Do not make up your own entites if it is not present\n",
    "    -Only strictly do what is asked to you. Do not give any explanations to your output.\n",
    "    <|end_of_text|>\n",
    "    <|start_of_role|>assistant<|end_of_role|>   \n",
    "    \"\"\"\n",
    "    extract_prompt = PromptTemplate(\n",
    "        input_variables = [\"page_content\"],\n",
    "        template = template,\n",
    "    )\n",
    "\n",
    "    entities = extract_prompt | llm | StrOutputParser()\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the model with the prompt to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = extract_entities_prompt_only()\n",
    "response = chain.invoke({\"page_content\": page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the response from the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - All entities defined in classes and then using pydantic to convert to openai function. This is then passed onto the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This import is helpful when we want to create structured outputs (like JSON responses) from OpenAI models using predefined schemas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining all the entities in a class along with the descripiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTaxProfit(BaseModel):\n",
    "    pre_tax_profit_percentage: str = Field(description=\"Operating pre-tax profit in percentage.\")\n",
    "    pre_tax_profit_numbers: str = Field(description=\"Operating pre-tax profit in numbers.\")\n",
    "\n",
    "\n",
    "class RevenueGrowth(BaseModel):\n",
    "    total_revenue_transaction_processing: str = Field(description=\"Total revenue growth for Transaction Processing sector in percentage.\")\n",
    "    total_revenue_data_ai: str = Field(description=\"Total revenue growth for Data and AI sector in percentage.\")\n",
    "    total_revenue_security: str = Field(description=\"Total revenue growth/decline for security sector in percentage.\")\n",
    "    total_revenue_automation: str = Field(description=\"Total revenue growth/decline for automation sector in percentage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping all the classes into one parent class which is given to pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarningCallReport(BaseModel):\n",
    "    company_name: str = Field(description=\"The public company name.\")\n",
    "    ceo_name: str = Field(description=\"Name of the CEO of the company.\")\n",
    "    pre_tax_profit: PreTaxProfit = Field(description=\"Operating pre-tax profit.\")\n",
    "    revenue: RevenueGrowth = Field(description=\"All revenue growth details for all sectors.\")\n",
    "    names_of_people: str = Field(description=\"All the names of the people that were mentioned in the transcript.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_tagging_function = convert_pydantic_to_openai_function(EarningCallReport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_tagging_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same prompt as before, but here, the pydantic function is passed here instead of defining each entity in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities():\n",
    "    template = \"\"\"\n",
    "    <|start_of_role|>user<|end_of_role|>\n",
    "    -You are AI Entity Extractor. You help extracting entities from the given transcript: {page_content}\n",
    "    -Analyze this transcript and extract the following entities as per the following function defination: {function_content}\n",
    "    -Your output should strictly be in a json format.\n",
    "    -Do not generate random entities on your own. If it is not present or you are unable to find any specified entity, you strictly have to output it as `Data not available`.\n",
    "    -Only do what is asked to you. Do not give any explanations to your output and do not hallucinate.\n",
    "    <|end_of_text|>\n",
    "    <|start_of_role|>assistant<|end_of_role|>   \n",
    "    \"\"\"\n",
    "    extract_prompt = PromptTemplate(\n",
    "        input_variables = [\"page_content\"],\n",
    "        template = template,\n",
    "    )\n",
    "\n",
    "    entities = extract_prompt | llm | StrOutputParser()\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the model with prompt to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = extract_entities()\n",
    "response = chain.invoke({\"page_content\": page_content, \"function_content\": overview_tagging_function})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the response from the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-enablement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
